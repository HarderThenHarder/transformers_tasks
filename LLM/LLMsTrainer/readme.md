# ğŸ¦™ LLMs Trainer

LLMs Trainer æ˜¯ä¸€ä¸ªæ—¨åœ¨å¸®åŠ©äººä»¬ä»é›¶å¼€å§‹è®­ç»ƒå¤§æ¨¡å‹çš„ä»“åº“ï¼Œè¯¥ä»“åº“æœ€æ—©å‚è€ƒè‡ª [Open-Llama](https://github.com/beichao1314/Open-Llama)ï¼Œå¹¶åœ¨å…¶åŸºç¡€ä¸Šè¿›è¡Œæ‰©å……ã€‚

<br>

ç›®å‰ï¼Œè¯¥ä»“åº“å°†æä¾›ä»¥ä¸‹èƒ½åŠ›ï¼š

- [x] ç»§ç»­é¢„è®­ç»ƒï¼ˆContinue Pretrainingï¼‰
- [x] æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰
- [x] å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼ˆReward Modelï¼‰
- [ ] å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰

<br>

æœ‰å…³ LLM è®­ç»ƒæµç¨‹çš„æ›´å¤šç»†èŠ‚å¯ä»¥å‚è€ƒ [è¿™ç¯‡æ–‡ç« ](https://zhuanlan.zhihu.com/p/636270877)ã€‚


ä½¿ç”¨ä»“åº“ä¹‹å‰ï¼Œè¯·å…ˆå®‰è£…æ‰€æœ‰éœ€è¦çš„ä¾èµ–ï¼š

```sh
pip install -r requirements.txt
```

---

## 1. ç»§ç»­é¢„è®­ç»ƒï¼ˆContinue Pretrainingï¼‰

ç»§ç»­é¢„è®­ç»ƒæ˜¯æŒ‡ï¼Œåœ¨ä¸€ä¸ªå·²æœ‰çš„æ¨¡å‹ä¸Šç»§ç»­è¿›è¡Œé¢„è®­ç»ƒå¢å¼ºï¼Œé€šå¸¸ç”¨äº `è‹±æ–‡æ¨¡å‹çš„ä¸­æ–‡å¢å¼º` æˆ–æ˜¯ `é¢†åŸŸæ•°æ®å¢å¼º`ã€‚

æˆ‘ä»¬è¿™é‡Œä»¥è‹±æ–‡æ¨¡å‹ [OpenLlama](https://huggingface.co/openlm-research/open_llama_7b_v2) åœ¨ä¸­æ–‡æ•°æ®é›† [MNBVC](https://huggingface.co/datasets/liwu/MNBVC) ä¸­çš„ **å°‘é‡æ•°æ®** ä¸ºä¾‹æ¥æ¼”ç¤ºæ•´ä¸ªæµç¨‹ã€‚


### 1.1 æ•°æ®å‹ç¼©

ç”±äºé¢„è®­ç»ƒæ•°æ®é›†é€šå¸¸æ¯”è¾ƒåºå¤§ï¼Œå› æ­¤å…ˆå°†è®­ç»ƒæ•°æ®è¿›è¡Œå‹ç¼©å¹¶æµæ°è¯»å–ã€‚

é¦–å…ˆï¼Œè¿›å…¥åˆ° `data` ç›®å½•:

```sh
cd data
```

æ‰¾åˆ°ç›®å½•ä¸‹çš„ `compress_data.py`, åœ¨è¯¥æ–‡ä»¶ä¸­ä¿®æ”¹éœ€è¦å‹ç¼©çš„æ•°æ®è·¯å¾„ï¼š

```python
SHARD_SIZE = 10      # å•ä¸ªæ–‡ä»¶å­˜æ”¾æ ·æœ¬çš„æ•°é‡, ç¤ºä¾‹ä¸­ä½¿ç”¨å¾ˆå°ï¼ŒçœŸå®è®­ç»ƒå¯ä»¥é…Œæƒ…å¢å¤§
...

def batch_compress_preatrain_data():
    """
    æ‰¹é‡å‹ç¼©é¢„è®­ç»ƒæ•°æ®ã€‚
    """
    source_path = 'shuffled_data/pretrain'                  # æºæ•°æ®æ–‡ä»¶
    target_path = 'pretrain_data'                           # å‹ç¼©åå­˜æ”¾åœ°å€

    files = [                                               # è¿™ä¸‰ä¸ªæ–‡ä»¶æ˜¯ç¤ºä¾‹æ•°æ®
        'MNBVC_news',
        'MNBVC_qa',
        'MNBVC_wiki'
    ]
    ...

if __name__ == '__main__':
    batch_compress_preatrain_data()
    # batch_compress_sft_data()
```
> Notes: ä¸Šè¿°çš„ files å¯ä»¥åœ¨ shuffled_data/pretrain/ ä¸­æ‰¾åˆ°ï¼Œæ˜¯æˆ‘ä»¬å‡†å¤‡çš„å°‘é‡ç¤ºä¾‹æ•°æ®ï¼ŒçœŸå®è®­ç»ƒä¸­è¯·æ›¿æ¢ä¸ºå®Œæ•´æ•°æ®ã€‚

åœ¨ `data` è·¯å¾„ä¸­æ‰§è¡Œ `python compress_data.py`, ç»ˆç«¯å°†æ˜¾ç¤ºï¼š

```sh
processed shuffled_data/pretrain/MNBVC_news.jsonl...
total line: 100
total files: 10
processed shuffled_data/pretrain/MNBVC_qa.jsonl...
total line: 50
total files: 5
processed shuffled_data/pretrain/MNBVC_wiki.jsonl...
total line: 100
total files: 10
```

éšåå¯åœ¨ `pretrain_data` ä¸­æ‰¾åˆ°å¯¹åº”çš„ `.jsonl.zst` å‹ç¼©æ–‡ä»¶ï¼ˆè¯¥è·¯å¾„å°†åœ¨ä¹‹åçš„è®­ç»ƒä¸­ä½¿ç”¨ï¼‰ã€‚


### 1.2 æ•°æ®æºé‡‡æ ·æ¯”ä¾‹ï¼ˆå¯é€‰ï¼‰

ä¸ºäº†æ›´å¥½çš„è¿›è¡Œä¸åŒæ•°æ®æºçš„é‡‡æ ·ï¼Œæˆ‘ä»¬æä¾›äº†æŒ‰ç…§é¢„è®¾æ¯”ä¾‹è¿›è¡Œæ•°æ®é‡‡æ ·çš„åŠŸèƒ½ã€‚

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¯è§†åŒ–å·¥å…·ç”¨äºè°ƒæ•´ä¸åŒæ•°æ®æºä¹‹é—´çš„åˆ†å¸ƒï¼Œåœ¨ `æ ¹ç›®å½•` ä¸‹ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ï¼š

```sh
streamlit run utils/sampler_viewer/web.py --server.port 8001
```

éšååœ¨æµè§ˆå™¨ä¸­è®¿é—® `æœºå™¨IP:8001` å³å¯æ‰“å¼€å¹³å°ã€‚

æˆ‘ä»¬æŸ¥çœ‹ `data/shuffled_data/pretrain` ä¸‹å„æ•°æ®çš„åŸå§‹æ–‡ä»¶å¤§å°ï¼š

```sh
-rw-r--r--@ 1 xx  staff   253K Aug  2 16:38 MNBVC_news.jsonl
-rw-r--r--@ 1 xx  staff   121K Aug  2 16:38 MNBVC_qa.jsonl
-rw-r--r--@ 1 xx  staff   130K Aug  2 16:37 MNBVC_wiki.jsonl
```

å¹¶å°†æ–‡ä»¶å¤§å°æŒ‰ç…§æ ¼å¼è´´åˆ°å¹³å°ä¸­ï¼š

<div align=center><img src='assets/sampler_viewer.jpeg'></div>

è°ƒæ•´å®Œæ¯•åï¼Œå¤åˆ¶ä¸Šå›¾å³ä¸‹è§’çš„æœ€ç»ˆæ¯”ä¾‹ï¼Œä¾¿äºåç»­è®­ç»ƒä½¿ç”¨ã€‚


### 1.3 è¯è¡¨æ‰©å……ï¼ˆå¯é€‰ï¼‰

ç”±äºåŸå§‹ Llama çš„ä¸­æ–‡ token å¾ˆå°‘ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€‰æ‹©å¯¹åŸæœ‰çš„ tokenizer è¿›è¡Œè¯è¡¨æ‰©å……ã€‚

è¿›å…¥åˆ° `utils` ç›®å½•ï¼š

```sh
cd utils
```

ä¿®æ”¹æ–‡ä»¶ `train_tokenizer.py` ä¸­çš„è®­ç»ƒæ•°æ®ï¼ˆæˆ‘ä»¬ä½¿ç”¨æ­£å¼é¢„è®­ç»ƒè®­ç»ƒæ•°æ®é›†ä½œä¸ºè®­ç»ƒè¯è¡¨çš„æ•°æ®é›†ï¼‰ï¼š

```python
...
dataset = {
    "MNBVC_news": "../data/pretrain_data/MNBVC_news/*.jsonl.zst",
    "MNBVC_qa": "../data/pretrain_data/MNBVC_qa/*.jsonl.zst",
    "MNBVC_wiki": "../data/pretrain_data/MNBVC_wiki/*.jsonl.zst",
}
```

æ‰§è¡Œå®Œ `train_tokenizer.py` åï¼Œè·¯å¾„ä¸‹ä¼šå‡ºç°è®­ç»ƒå¥½çš„æ¨¡å‹ `test_tokenizer.model`ã€‚

éšåï¼Œæˆ‘ä»¬å°†è®­ç»ƒå¥½çš„ model å’ŒåŸæœ¬çš„ llama model åšèåˆï¼š

```sh
python merge_tokenizer.py
```

ä½ å¯ä»¥ä½¿ç”¨ [è¿™ä¸ªå·¥å…·](https://github.com/HarderThenHarder/transformers_tasks/tree/main/tools/tokenizer_viewer) å¾ˆæ–¹ä¾¿çš„å¯¹åˆå¹¶å¥½åçš„ tokenizer è¿›è¡Œå¯è§†åŒ–ã€‚


### 1.4 å¹³å‡åˆå§‹åŒ– extend token embeddingï¼ˆå¯é€‰ï¼‰

ä¸ºäº†å‡å°æ‰©å±•çš„ token embedding éšæœºåˆå§‹åŒ–å¸¦æ¥æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œæˆ‘ä»¬æä¾›ä½¿ç”¨å°†æ–° token åœ¨åŸ tokenizer ä¸­çš„ sub-token embedding çš„å¹³å‡å€¼åšä¸ºåˆå§‹åŒ– embedding çš„æ–¹æ³•ã€‚

å…·ä½“ä½¿ç”¨æ–¹æ³•åœ¨ `utils/extend_model_token_embeddings.py`ã€‚


### 1.5 æ­£å¼è®­ç»ƒ

å½“å®Œæˆä¸Šè¿°æ­¥éª¤åå°±å¯ä»¥å¼€å§‹æ­£å¼è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒï¼š

```sh
sh train_llms.sh configs/accelerate_configs/ds_stage1.yaml \
    configs/pretrain_configs/llama.yaml \
    openlm-research/open_llama_7b_v2
```

å¤šæœºå¤šå¡åˆ™å¯åŠ¨ï¼š

```sh
sh train_multi_node_reward_model.sh configs/accelerate_configs/ds_stage1.yaml \
    configs/pretrain_configs/llama.yaml \
    openlm-research/open_llama_7b_v2
```

æ³¨æ„ï¼Œæ‰€æœ‰çš„è®­ç»ƒé…ç½®éƒ½æ”¾åœ¨äº†ç¬¬ 2 ä¸ªå‚æ•° `configs/pretrain_configs/llama.yaml` ä¸­ï¼Œæˆ‘ä»¬æŒ‘å‡ ä¸ªé‡è¦çš„å‚æ•°ä»‹ç»ã€‚

* `tokenizer_path (str)`ï¼štokenizer åŠ è½½è·¯å¾„ã€‚

* `ckpt (str)`ï¼šåˆå§‹ model åŠ è½½è·¯å¾„ã€‚

* `sample_policy_file (str)`ï¼šæ•°æ®æºé‡‡æ ·é…ç½®æ–‡ä»¶ï¼Œè‹¥ä¸åŒ…å«è¿™ä¸€é¡¹åˆ™ä¸è¿›è¡Œæ•°æ®æºé‡‡æ ·ã€‚

* `train_and_eval (bool)`ï¼šè¯¥å‚æ•°å†³å®šäº†æ˜¯å¦åœ¨è®­ç»ƒä¸­æ‰§è¡Œè¯„ä¼°å‡½æ•°ã€‚

* `img_log_dir (str)`ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­çš„ log å›¾å­˜æ”¾ç›®å½•ã€‚

* `eval_methods (list)`ï¼šä½¿ç”¨å“ªäº›è¯„ä¼°å‡½æ•°ï¼ŒåŒ…æ‹¬ï¼š
    
    * single_choice_eval: å•é€‰é¢˜æ­£ç¡®ç‡æµ‹è¯•ï¼ˆå¦‚: [C-Eval](https://github.com/SJTU-LIT/ceval)ï¼‰ï¼Œè¯„ä¼°æ•°æ®æ ¼å¼å‚è€ƒ `eval_data/knowledge/knowledge_and_reasoning.jsonl`ã€‚

    * generation_eval: ç”Ÿæˆæµ‹è¯•ï¼Œç»™å®š promptï¼Œæµ‹è¯•æ¨¡å‹ç”Ÿæˆèƒ½åŠ›ï¼Œè¯„ä¼°æ•°æ®æ ¼å¼å‚è€ƒ `eval_data/pretrain/generation_test.jsonl`ã€‚

* `work_dir (str)`ï¼šè®­ç»ƒæ¨¡å‹å­˜æ”¾è·¯å¾„ã€‚

* `save_total_limit (int)`ï¼šæœ€å¤šä¿å­˜çš„æ¨¡å‹ä¸ªæ•°ï¼ˆè¶…è¿‡æ•°ç›®åˆ™åˆ é™¤æ—§çš„æ¨¡å‹ï¼‰



## 2. æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰

æˆ‘ä»¬å‡†å¤‡äº†éƒ¨åˆ† `ShareGPT` çš„æ•°æ®ä½œä¸ºç¤ºä¾‹æ•°æ®ï¼Œæˆ‘ä»¬ä»æ—§ä½¿ç”¨ [OpenLlama](https://huggingface.co/openlm-research/open_llama_7b_v2) ä½œä¸ºè®­ç»ƒçš„åŸºåº§æ¨¡å‹ã€‚


### 2.1 æ•°æ®å‹ç¼©

åŒé¢„è®­ç»ƒä¸€æ ·ï¼Œæˆ‘ä»¬å…ˆè¿›å…¥åˆ° `data` ç›®å½•:

```sh
cd data
```

æ‰¾åˆ°ç›®å½•ä¸‹çš„ `compress_data.py`, åœ¨è¯¥æ–‡ä»¶ä¸­ä¿®æ”¹éœ€è¦å‹ç¼©çš„æ•°æ®è·¯å¾„ï¼š

```python
SHARD_SIZE = 10      # å•ä¸ªæ–‡ä»¶å­˜æ”¾æ ·æœ¬çš„æ•°é‡, ç¤ºä¾‹ä¸­ä½¿ç”¨å¾ˆå°ï¼ŒçœŸå®è®­ç»ƒå¯ä»¥é…Œæƒ…å¢å¤§
...

def batch_compress_sft_data():
    """
    æ‰¹é‡å‹ç¼©SFTæ•°æ®ã€‚
    """
    source_path = 'shuffled_data/sft'
    target_path = 'sft_data'

    files = [
        'sharegpt'
    ]
    ...

if __name__ == '__main__':
    # batch_compress_preatrain_data()
    batch_compress_sft_data()
```
> Notes: ä¸Šè¿°çš„ files å¯ä»¥åœ¨ shuffled_data/sft/ ä¸­æ‰¾åˆ°ï¼Œæ˜¯æˆ‘ä»¬å‡†å¤‡çš„å°‘é‡ç¤ºä¾‹æ•°æ®ï¼ŒçœŸå®è®­ç»ƒä¸­è¯·æ›¿æ¢ä¸ºå®Œæ•´æ•°æ®ã€‚

åœ¨ `data` è·¯å¾„ä¸­æ‰§è¡Œ `python compress_data.py`, ç»ˆç«¯å°†æ˜¾ç¤ºï¼š

```sh
processed shuffled_data/sft/sharegpt.jsonl...
total line: 9637
total files: 964
```

éšåå¯åœ¨ `sft_data` ä¸­æ‰¾åˆ°å¯¹åº”çš„ `.jsonl.zst` å‹ç¼©æ–‡ä»¶ï¼ˆè¯¥è·¯å¾„å°†åœ¨ä¹‹åçš„è®­ç»ƒä¸­ä½¿ç”¨ï¼‰ã€‚


### 2.2 ç‰¹æ®Š token æ‰©å……

å—åˆ° [ChatML](https://github.com/openai/openai-python/blob/main/chatml.md) çš„å¯å‘ï¼Œæˆ‘ä»¬éœ€è¦åœ¨åŸæœ‰çš„ tokenizer ä¸­æ·»åŠ ä¸€äº› special token ç”¨äºå¯¹è¯ç³»ç»Ÿã€‚

ä¸€ç§æœ€ç®€å•çš„æ–¹å¼æ˜¯åœ¨ tokenizer è·¯å¾„ä¸­æ‰¾åˆ° `special_tokens_map.json` æ–‡ä»¶ï¼Œå¹¶æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š

```python
{
    ...                                         # éœ€è¦æ·»åŠ çš„ç‰¹æ®Š token
    "system_token": "<|system|>",               # system prompt
    "user_token": "<|user|>",                   # user token
    "assistant_token": "<|assistant|>",         # chat-bot token
    "chat_end_token": "<|endofchat|>"           # chat end token
}
```


### 2.3 å¾®è°ƒè®­ç»ƒ

å½“å®Œæˆä¸Šè¿°æ­¥éª¤åå°±å¯ä»¥å¼€å§‹æ­£å¼è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒï¼š

```sh
sh train_llms.sh configs/accelerate_configs/ds_stage1.yaml \
    configs/sft_configs/llama.yaml \
    openlm-research/open_llama_7b_v2
```

å¤šæœºå¤šå¡åˆ™å¯åŠ¨ï¼š

```sh
sh train_multi_node_reward_model.sh configs/accelerate_configs/ds_stage1.yaml \
    configs/sft_configs/llama.yaml \
    openlm-research/open_llama_7b_v2
```

æ³¨æ„ï¼Œæ‰€æœ‰çš„è®­ç»ƒé…ç½®éƒ½æ”¾åœ¨äº†ç¬¬ 2 ä¸ªå‚æ•° `configs/sft_configs/llama.yaml` ä¸­ï¼Œæˆ‘ä»¬æŒ‘å‡ ä¸ªé‡è¦çš„å‚æ•°ä»‹ç»ã€‚

* `tokenizer_path (str)`ï¼štokenizer åŠ è½½è·¯å¾„ã€‚

* `ckpt (str)`ï¼šåˆå§‹ model åŠ è½½è·¯å¾„ã€‚

* `train_and_eval (bool)`ï¼šè¯¥å‚æ•°å†³å®šäº†æ˜¯å¦åœ¨è®­ç»ƒä¸­æ‰§è¡Œè¯„ä¼°å‡½æ•°ã€‚

* `img_log_dir (str)`ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­çš„ log å›¾å­˜æ”¾ç›®å½•ã€‚

* `eval_methods (list)`ï¼šä½¿ç”¨å“ªäº›è¯„ä¼°å‡½æ•°ï¼ŒåŒ…æ‹¬ï¼š

    * generation_eval: ç”Ÿæˆæµ‹è¯•ï¼Œç»™å®š promptï¼Œæµ‹è¯•æ¨¡å‹ç”Ÿæˆèƒ½åŠ›ï¼Œè¯„ä¼°æ•°æ®æ ¼å¼å‚è€ƒ `eval_data/sft/share_gpt_test.jsonl`ã€‚

    * æš‚æ— ã€‚

* `work_dir (str)`ï¼šè®­ç»ƒæ¨¡å‹å­˜æ”¾è·¯å¾„ã€‚

* `save_total_limit (int)`ï¼šæœ€å¤šä¿å­˜çš„æ¨¡å‹ä¸ªæ•°ï¼ˆè¶…è¿‡æ•°ç›®åˆ™åˆ é™¤æ—§çš„æ¨¡å‹ï¼‰



## 3. å¥–åŠ±æ¨¡å‹ï¼ˆReward Modelï¼‰


### 3.1 æ•°æ®é›†å‡†å¤‡

æˆ‘ä»¬å‡†å¤‡ 1000 æ¡ååºå¯¹ä½œä¸ºç¤ºä¾‹è®­ç»ƒæ•°æ®ï¼Œå…¶ä¸­ `selected` ä¸ºä¼˜åŠ¿æ ·æœ¬ï¼Œ`rejected` ä¸ºåŠ£åŠ¿æ ·æœ¬ï¼š

```python
{
    "prompt": "ä¸‹é¢æ˜¯ä¸€æ¡æ­£é¢çš„è¯„è®ºï¼š",
    "selected": "å¾ˆå¥½ç”¨ï¼Œä¸€ç“¶éƒ½ç”¨å®Œäº†æ‰æ¥è¯„ä»·ã€‚",
    "rejected": "æ‰¾äº†å¾ˆä¹…å¤§å°åŒ…è£…éƒ½æ²¡æ‰¾åˆ°ç”Ÿäº§æ—¥æœŸã€‚ä¸Šå½“äº†ã€‚"
}
```

è¿™ä¸ªæ­¥éª¤ä¸å†éœ€è¦æ•°æ®å‹ç¼©ï¼Œå› æ­¤å‡†å¤‡å¥½ä¸Šè¿°ç»“æ„çš„ `.jsonl` æ–‡ä»¶å³å¯ã€‚


### 3.2 RM è®­ç»ƒ

å½“å®Œæˆä¸Šè¿°æ­¥éª¤åå°±å¯ä»¥å¼€å§‹æ­£å¼è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒï¼š

```sh
sh train_multi_node_reward_model.sh \
    configs/accelerate_configs/ds_stage1.yaml \
    configs/reward_model_configs/llama7b.yaml
```

æ³¨æ„ï¼Œæ‰€æœ‰çš„è®­ç»ƒé…ç½®éƒ½æ”¾åœ¨äº†ç¬¬ 2 ä¸ªå‚æ•° `configs/reward_model_configs/llama.yaml` ä¸­ï¼Œæˆ‘ä»¬æŒ‘å‡ ä¸ªé‡è¦çš„å‚æ•°ä»‹ç»ã€‚

* `tokenizer_path (str)`ï¼štokenizer åŠ è½½è·¯å¾„ã€‚

* `ckpt (str)`ï¼šåˆå§‹ model åŠ è½½è·¯å¾„ã€‚

* `train_and_eval (bool)`ï¼šè¯¥å‚æ•°å†³å®šäº†æ˜¯å¦åœ¨è®­ç»ƒä¸­æ‰§è¡Œè¯„ä¼°å‡½æ•°ã€‚

* `img_log_dir (str)`ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­çš„ log å›¾å­˜æ”¾ç›®å½•ã€‚

* `test_reward_model_acc_files (list)`ï¼šacc æµ‹è¯•æ–‡ä»¶åˆ—è¡¨ã€‚

* `work_dir (str)`ï¼šè®­ç»ƒæ¨¡å‹å­˜æ”¾è·¯å¾„ã€‚

* `save_total_limit (int)`ï¼šæœ€å¤šä¿å­˜çš„æ¨¡å‹ä¸ªæ•°ï¼ˆè¶…è¿‡æ•°ç›®åˆ™åˆ é™¤æ—§çš„æ¨¡å‹ï¼‰