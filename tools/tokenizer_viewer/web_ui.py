# !/usr/bin/env python3
"""
==== No Bugs in code, just some Random Unexpected FEATURES ====
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”â”‚
â”‚â”‚Escâ”‚!1 â”‚@2 â”‚#3 â”‚$4 â”‚%5 â”‚^6 â”‚&7 â”‚*8 â”‚(9 â”‚)0 â”‚_- â”‚+= â”‚|\ â”‚`~ â”‚â”‚
â”‚â”œâ”€â”€â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”€â”€â”¤â”‚
â”‚â”‚ Tab â”‚ Q â”‚ W â”‚ E â”‚ R â”‚ T â”‚ Y â”‚ U â”‚ I â”‚ O â”‚ P â”‚{[ â”‚}] â”‚ BS  â”‚â”‚
â”‚â”œâ”€â”€â”€â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”€â”€â”€â”€â”€â”¤â”‚
â”‚â”‚ Ctrl â”‚ A â”‚ S â”‚ D â”‚ F â”‚ G â”‚ H â”‚ J â”‚ K â”‚ L â”‚: ;â”‚" 'â”‚ Enter  â”‚â”‚
â”‚â”œâ”€â”€â”€â”€â”€â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”¤â”‚
â”‚â”‚ Shift  â”‚ Z â”‚ X â”‚ C â”‚ V â”‚ B â”‚ N â”‚ M â”‚< ,â”‚> .â”‚? /â”‚Shift â”‚Fn â”‚â”‚
â”‚â””â”€â”€â”€â”€â”€â”¬â”€â”€â”´â”¬â”€â”€â”´â”€â”€â”¬â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”¬â”´â”€â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”˜â”‚
â”‚      â”‚Fn â”‚ Alt â”‚         Space         â”‚ Alt â”‚Winâ”‚   HHKB   â”‚
â”‚      â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å¯è§†åŒ–tokenizerã€‚

Author: pankeyu
Date: 2023/06/01
"""
import pandas as pd
import streamlit as st
from transformers import AutoTokenizer


st.set_page_config(
    page_title="Tokenizer Viewer",
)

SAVED_PATH = 'tokenizers/saved'
CACHED_PATH = 'tokenizers/cached'


if 'current_tokenizer' not in st.session_state:
    st.session_state['current_tokenizer'] = None
    st.session_state['current_tokenizer_vocab'] = {}
    st.session_state['current_tokenizer_added_vocab'] = {}


if 'diff_tokenizer_1' not in st.session_state:
    st.session_state['diff_tokenizer_1'] = None
    st.session_state['diff_tokenizer_1_vocab'] = {}
    st.session_state['diff_tokenizer_1_added_vocab'] = {}
    st.session_state['diff_tokenizer_2'] = None
    st.session_state['diff_tokenizer_2_vocab'] = {}
    st.session_state['diff_tokenizer_2_added_vocab'] = {}


def load_tokenizer(
        token_path_or_name: str
    ):
    """
    åŠ è½½æŒ‡å®šçš„tokenizerã€‚
    """
    return AutoTokenizer.from_pretrained(
        token_path_or_name,
        trust_remote_code=True
    )
    

def show_vocab_area(
        vocab: dict,
        vocab_df: pd.DataFrame,
        source: str
    ):
    """
    å±•ç¤ºæŒ‡å®šçš„vocabã€‚

    Args:
        vocab (dict): _description_
    """
    search_token = st.text_input(
        f'åœ¨ {source} ä¸­æœç´¢æŒ‡å®š token',
        placeholder=f'åœ¨ {source} ä¸­æœç´¢æŒ‡å®š token...',
        label_visibility='collapsed'
    )

    if search_token:
        token_idx = (
            -1 
            if search_token not in vocab 
            else vocab[search_token]
        )

        if token_idx != -1:
            st.success(f'Token ã€Œ{search_token}ã€ å­˜åœ¨äºå½“å‰è¯è¡¨ä¸­ï¼Œtoken_idx ä¸º {token_idx}ã€‚')
        else:
            st.warning(f'Token ã€Œ{search_token}ã€ ä¸å­˜åœ¨äºå½“å‰è¯è¡¨ä¸­ã€‚')
    
    st.dataframe(
        vocab_df, 
        use_container_width=True,
        height=600
    )


def show_tokenize_area():
    """
    è¿›è¡Œ tokenizer encode/decode æµ‹è¯•ã€‚
    """
    with st.expander('ç¼–/è§£ç æµ‹è¯•', expanded=True):
        text = st.text_input(
            '[>>>] Encoding Test: ',
            value='è¿™æ˜¯ä¸€ä¸ªå¾…ç¼–ç çš„å¥å­'
        )

        if text:
            ids = st.session_state['current_tokenizer'](text)['input_ids']
            tokens = st.session_state['current_tokenizer'].convert_ids_to_tokens(ids)
            st.code(f'encode tokenï¼š{tokens}\nencode token idsï¼š{ids}')
        
        id_list = st.text_input(
            '[<<<] Decoding Test: ',
            value='[0, 1, 2]'
        )

        if id_list:
            try:
                id_list = eval(id_list)
                tokens = st.session_state['current_tokenizer'].convert_ids_to_tokens(id_list)
                tokens2 = st.session_state['current_tokenizer'].decode(id_list)
                st.code(f'convert_ids_to_tokens ç»“æœï¼š{tokens}')
                st.code(f'decode ç»“æœï¼š{tokens2}')
            except Exception as e:
                st.error(e)


def get_vocab_dataframe(vocab: dict):
    """
    å°†vocabè½¬æ¢æˆdfå½¢å¼ã€‚

    Args:
        vocab (dict): _description_
    """
    temp_dict = {
        'token': list(vocab.keys()),
        'idx': list(vocab.values())
    }
    return pd.DataFrame.from_dict(temp_dict)


def sort_vocab_by_index(vocab_dict: dict):
    """
    å°† vocab_dict æŒ‰ç…§ index è¿›è¡Œæ’åºã€‚
    """
    return dict(sorted(vocab_dict.items(), key=lambda x: x[1]))


def start_viewer_page():
    """
    å¯è§†åŒ–tokenizerã€‚
    """
    st.code("""è¯¥é¡µé¢ç”¨äºå¯è§†åŒ– tokenizer ä¿¡æ¯ï¼Œä»¥å¸®åŠ©æ›´å¿«é€Ÿï¼Œæ›´ç›´è§‚çš„äº†è§£ tokenizerï¼ŒåŒ…å«ä»¥ä¸‹åŠŸèƒ½ï¼š
1. æŸ¥çœ‹ tokenizer åŒ…å«å­—ç¬¦æ•°ï¼ˆåŸå§‹å­—ç¬¦æ•°ã€é¢å¤–æ‰©å±•å­—ç¬¦æ•°ï¼‰ã€‚
2. æœç´¢ tokenizer ä¸­æ˜¯å¦åŒ…å«æŒ‡å®š tokenã€‚
3. encode å’Œ decode åŠŸèƒ½æµ‹è¯•ã€‚
    """)

    st.write('tokenizer è·¯å¾„ï¼ˆhuggingface æˆ– å¼€å‘æœº ç›®å½•ï¼‰:')

    c1, c2 = st.columns([10, 4])
    with c1:
        token_path_or_name = st.text_input(
            'tokenizer è·¯å¾„ï¼ˆhuggingface æˆ– å¼€å‘æœº ç›®å½•ï¼‰:',
            value='bert-base-chinese',
            label_visibility='collapsed'
        )
    
    with c2:
        btn = st.button('Load Tokenizer!')

    if btn:
        with st.spinner('Loading tokenizer...'):
            st.session_state['current_tokenizer'] = load_tokenizer(token_path_or_name)
            st.session_state['current_tokenizer_vocab'] = sort_vocab_by_index(st.session_state['current_tokenizer'].get_vocab())
            st.session_state['current_tokenizer_vocab_list'] = list(st.session_state['current_tokenizer_vocab'].keys())
            st.session_state['current_tokenizer_vocab_df'] = get_vocab_dataframe(st.session_state['current_tokenizer_vocab'])
            st.session_state['current_tokenizer_added_vocab'] = sort_vocab_by_index(st.session_state['current_tokenizer'].get_added_vocab())
            st.session_state['current_tokenizer_added_vocab_list'] = list(st.session_state['current_tokenizer_added_vocab'].keys())
            st.session_state['current_tokenizer_added_vocab_df'] = get_vocab_dataframe(st.session_state['current_tokenizer_added_vocab'])

    if st.session_state['current_tokenizer']:
        with st.expander('å­—ç¬¦æ•°ç»Ÿè®¡', expanded=True):
            temp_dict = {
                'tokenizer': [token_path_or_name],
                'vocab_len': [len(st.session_state['current_tokenizer_vocab']) - len(st.session_state['current_tokenizer_added_vocab'])],
                'added_vocab_len': [len(st.session_state['current_tokenizer_added_vocab'])],
                'all_vocab_len': [len(st.session_state['current_tokenizer_vocab'])]
            }
            df = pd.DataFrame.from_dict(temp_dict)
            st.dataframe(df, use_container_width=True)
    
        c1, c2 = st.columns([5, 5])
        
        with c1:
            show_vocab_area(
                st.session_state['current_tokenizer_vocab'],
                st.session_state['current_tokenizer_vocab_df'],
                source='vocab'
            )

        with c2:
            show_vocab_area(
                st.session_state['current_tokenizer_added_vocab'],
                st.session_state['current_tokenizer_added_vocab_df'],
                source='added vocab'
            )

        show_tokenize_area()


def show_diff_area():
    """
    å·®åˆ†å‡ºä¸¤ä¸ªtokenizerä¸åŒçš„tokenã€‚
    """
    c1, c2 = st.columns([5, 5])

    tokenizer1_unique_token = set(st.session_state['diff_tokenizer_1_vocab_list']) - set(st.session_state['diff_tokenizer_2_vocab_list'])
    tokenizer2_unique_token = set(st.session_state['diff_tokenizer_2_vocab_list']) - set(st.session_state['diff_tokenizer_1_vocab_list'])

    with c1:
        st.write(f'tokenizer 1 ç‹¬æœ‰çš„ tokensï¼ˆ{len(tokenizer1_unique_token)}ï¼‰ï¼š')
        temp_dict = {
            'more tokens': list(tokenizer1_unique_token)
        }
        st.dataframe(
            temp_dict,
            use_container_width=True,
            height=600
        )
    
    with c2:
        st.write(f'tokenizer 2 ç‹¬æœ‰çš„ tokensï¼ˆ{len(tokenizer2_unique_token)}ï¼‰ï¼š')
        temp_dict = {
            'more tokens': list(tokenizer2_unique_token)
        }
        st.dataframe(
            temp_dict,
            use_container_width=True,
            height=600
        )


def start_diff_page():
    """
    ç”¨äºæ¯”è¾ƒä¸¤ä¸ªtokenizerçš„ä¸åŒã€‚
    """
    st.code("""è¯¥é¡µé¢ç”¨äºæ¯”è¾ƒ 2 ä¸ª tokenizer ä¹‹é—´çš„ diff tokenï¼Œç”¨äºäº†è§£ tokenizer ä¹‹é—´çš„å·®å¼‚ï¼š
1. æŸ¥çœ‹ tokenizer ä¹‹é—´æœ‰å“ªäº› tokens ä¸åŒã€‚
2. å°† 2 ä¸ªtokenizer ä¹‹é—´åš mergeã€‚
    """)

    c1, c2, c3 = st.columns([10, 10, 6])
    with c1:
        st.write('tokenizer 1 è·¯å¾„ï¼ˆhuggingface æˆ– å¼€å‘æœº ç›®å½•ï¼‰:')
    with c2:
        st.write('tokenizer 2 è·¯å¾„ï¼ˆhuggingface æˆ– å¼€å‘æœº ç›®å½•ï¼‰:')

    c1, c2, c3 = st.columns([10, 10, 6])
    with c1:
        token_1_path_or_name = st.text_input(
            'tokenizer 1 è·¯å¾„ï¼ˆhuggingface æˆ– å¼€å‘æœº ç›®å½•ï¼‰:',
            value='bert-base-chinese',
            label_visibility='collapsed'
        )
    with c2:
        token_2_path_or_name = st.text_input(
            'tokenizer 2 è·¯å¾„ï¼ˆhuggingface æˆ– å¼€å‘æœº ç›®å½•ï¼‰:',
            value='tiiuae/falcon-7b',
            label_visibility='collapsed'
        )
    with c3:
        start_btn = st.button(
            'Loadï¼'
        )

    if start_btn:

        if token_1_path_or_name and token_2_path_or_name:
            with st.spinner('Loading tokenizer...'):
                st.session_state['diff_tokenizer_1'] = load_tokenizer(token_1_path_or_name)
                st.session_state['diff_tokenizer_1_vocab'] = sort_vocab_by_index(st.session_state['diff_tokenizer_1'].get_vocab())
                st.session_state['diff_tokenizer_1_vocab_list'] = list(st.session_state['diff_tokenizer_1_vocab'].keys())
                st.session_state['diff_tokenizer_1_vocab_df'] = get_vocab_dataframe(st.session_state['diff_tokenizer_1_vocab'])
                st.session_state['diff_tokenizer_1_added_vocab'] = sort_vocab_by_index(st.session_state['diff_tokenizer_1'].get_added_vocab())
                st.session_state['diff_tokenizer_1_added_vocab_list'] = list(st.session_state['diff_tokenizer_1_added_vocab'].keys())
                st.session_state['diff_tokenizer_1_added_vocab_df'] = get_vocab_dataframe(st.session_state['diff_tokenizer_1_added_vocab'])
                
                st.session_state['diff_tokenizer_2'] = load_tokenizer(token_2_path_or_name)
                st.session_state['diff_tokenizer_2_vocab'] = sort_vocab_by_index(st.session_state['diff_tokenizer_2'].get_vocab())
                st.session_state['diff_tokenizer_2_vocab_list'] = list(st.session_state['diff_tokenizer_2_vocab'].keys())
                st.session_state['diff_tokenizer_2_vocab_df'] = get_vocab_dataframe(st.session_state['diff_tokenizer_2_vocab'])
                st.session_state['diff_tokenizer_2_added_vocab'] = sort_vocab_by_index(st.session_state['diff_tokenizer_2'].get_added_vocab())
                st.session_state['diff_tokenizer_2_added_vocab_list'] = list(st.session_state['diff_tokenizer_2_added_vocab'].keys())
                st.session_state['diff_tokenizer_2_added_vocab_df'] = get_vocab_dataframe(st.session_state['diff_tokenizer_2_added_vocab'])
        else:
            st.warning('è¯·åŒæ—¶å¡«å†™ä¸¤ä¸ªè·¯å¾„ï¼')
    
    if st.session_state['diff_tokenizer_1']:
        with st.expander('å­—ç¬¦æ•°ç»Ÿè®¡', expanded=True):
            temp_dict = {
                'tokenizer': [
                    token_1_path_or_name, 
                    token_2_path_or_name,
                    'diff'
                ],
                'vocab_len': [
                    len(st.session_state['diff_tokenizer_1_vocab']) - len(st.session_state['diff_tokenizer_1_added_vocab']),
                    len(st.session_state['diff_tokenizer_2_vocab']) - len(st.session_state['diff_tokenizer_2_added_vocab']),
                    len(st.session_state['diff_tokenizer_1_vocab']) - len(st.session_state['diff_tokenizer_1_added_vocab']) \
                        - len(st.session_state['diff_tokenizer_2_vocab']) + len(st.session_state['diff_tokenizer_2_added_vocab'])
                ],
                'added_vocab_len': [
                    len(st.session_state['diff_tokenizer_1_added_vocab']),
                    len(st.session_state['diff_tokenizer_2_added_vocab']),
                    len(st.session_state['diff_tokenizer_1_added_vocab']) - len(st.session_state['diff_tokenizer_2_added_vocab'])
                ],
                'all_vocab_len': [
                    len(st.session_state['diff_tokenizer_1_vocab']),
                    len(st.session_state['diff_tokenizer_2_vocab']),
                    len(st.session_state['diff_tokenizer_1_vocab']) - len(st.session_state['diff_tokenizer_2_vocab'])

                ]
            }
            df = pd.DataFrame.from_dict(temp_dict)
            st.dataframe(df, use_container_width=True)
        
        show_diff_area()


def main():
    """
    ä¸»å‡½æ•°æµç¨‹ã€‚
    """
    st.markdown(
        "<h1 style='text-align: center;'>ğŸ¤— Tokenizer Viewer</h1>", 
        unsafe_allow_html=True
    )
    
    viewer_page, merge_page, train_page = st.tabs([
        'viewer_page',
        'diff_page',
        'train_page'
    ])
    
    with viewer_page:
        start_viewer_page()

    with merge_page:
        start_diff_page()

    with train_page:
        pass


if __name__ == '__main__':
    main()