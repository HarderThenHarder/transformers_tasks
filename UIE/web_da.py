# !/usr/bin/env python3
"""
==== No Bugs in code, just some Random Unexpected FEATURES ====
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”â”‚
â”‚â”‚Escâ”‚!1 â”‚@2 â”‚#3 â”‚$4 â”‚%5 â”‚^6 â”‚&7 â”‚*8 â”‚(9 â”‚)0 â”‚_- â”‚+= â”‚|\ â”‚`~ â”‚â”‚
â”‚â”œâ”€â”€â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”€â”€â”¤â”‚
â”‚â”‚ Tab â”‚ Q â”‚ W â”‚ E â”‚ R â”‚ T â”‚ Y â”‚ U â”‚ I â”‚ O â”‚ P â”‚{[ â”‚}] â”‚ BS  â”‚â”‚
â”‚â”œâ”€â”€â”€â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”´â”€â”€â”€â”€â”€â”¤â”‚
â”‚â”‚ Ctrl â”‚ A â”‚ S â”‚ D â”‚ F â”‚ G â”‚ H â”‚ J â”‚ K â”‚ L â”‚: ;â”‚" 'â”‚ Enter  â”‚â”‚
â”‚â”œâ”€â”€â”€â”€â”€â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”¬â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”¤â”‚
â”‚â”‚ Shift  â”‚ Z â”‚ X â”‚ C â”‚ V â”‚ B â”‚ N â”‚ M â”‚< ,â”‚> .â”‚? /â”‚Shift â”‚Fn â”‚â”‚
â”‚â””â”€â”€â”€â”€â”€â”¬â”€â”€â”´â”¬â”€â”€â”´â”€â”€â”¬â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”¬â”´â”€â”€â”€â”´â”¬â”€â”€â”´â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”˜â”‚
â”‚      â”‚Fn â”‚ Alt â”‚         Space         â”‚ Alt â”‚Winâ”‚   HHKB   â”‚
â”‚      â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è‡ªåŠ¨æ•°æ®å¢å¼º web serverã€‚

Author: pankeyu
Date: 2023/01/19
"""
import os
import json
import time
from io import StringIO
from typing import List

import torch
import pandas as pd
import streamlit as st
from transformers import AutoTokenizer, T5ForConditionalGeneration

from Augmenter import Augmenter
from inference import inference


st.set_page_config(
    page_title="Auto Data Augment For UIE",
    page_icon="ğŸ§¸"
)

device = 'cpu'                                                 # æŒ‡å®šè®¾å¤‡
generated_dataset_height = 800                                 # ç”Ÿæˆæ ·æœ¬å±•ç¤ºé«˜åº¦                
max_show_num = 500                                             # ç”Ÿæˆæ ·æœ¬æœ€å¤§ä¿å­˜è¡Œæ•°
max_seq_len = 128                                              # æ•°æ®é›†å•å¥æœ€å¤§é•¿åº¦
batch_size = 128                                               # è´Ÿä¾‹ç”Ÿæˆæ—¶çš„batch_size

filling_model_path = 'transformers_tasks/data_augment/mask_then_fill/checkpoints/t5/model_best'


def get_prompt_samples(name: str) -> List[str]:
    """
    è§£ææœåŠ¡å™¨/ç”¨æˆ·ä¸Šä¼ çš„æ•°æ®é›†ã€‚
    """
    st.markdown('é€‰æ‹© **:green[.txt]** æ•°æ®é›†ç”¨äºæ•°æ®å¢å¼ºï¼Œæ”¯æŒæ¥æ”¶å¤šä¸ªæ–‡ä»¶ã€‚')
    st.code('âš ï¸ æ³¨æ„ï¼šè‹¥ä¼ å…¥å¤šä¸ªæ–‡ä»¶ï¼Œå¤šä¸ªæ–‡ä»¶çš„æ•°æ®å°†è¢«èåˆã€‚')
    st.markdown('* æ•°æ®é›†ç¤ºä¾‹ï¼š')
    st.code("""[
        {
            "content": "ä¸€æ ä¸‰æ˜Ÿä»€ä¹ˆå†›è¡”ï¼Ÿä¸Šå°‰", 
            "prompt": "ä¸€æ ä¸‰æ˜Ÿçš„å†›è¡”", 
            "result_list": [{"text": "ä¸Šå°‰", "start": 9, "end": 11}]
        },
        ...
    ]
    """, language='json')
    
    with st.expander('', expanded=True):
        all_samples, files_count = [], 0
        upload_file = st.checkbox(f'ä¸Šä¼ æœ¬åœ°æ–‡ä»¶ç”¨äºï¼š{name}', help='ä¸Šä¼ æœ¬åœ°çš„æ•°æ®é›†æ–‡ä»¶ã€‚')
        if upload_file:
            uploaded_files = st.file_uploader(f"é€‰æ‹©æ–‡ä»¶", accept_multiple_files=True)
            for uploaded_file in uploaded_files:
                stringio = StringIO(uploaded_file.getvalue().decode("utf-8"))
                string_data = stringio.read()
                cur_data = string_data.split('\n')
                all_samples.extend([json.loads(line) for line in cur_data])
                files_count += 1
                df_dict = {
                    'text': cur_data[:5] + ['...']
                }
                df = pd.DataFrame.from_dict(df_dict)
                st.dataframe(df)
        else:
            dataset_path_str = st.text_input(f'ã€Œè®­ç»ƒæ•°æ®é›†ã€çš„æœåŠ¡å™¨è·¯å¾„ï¼ˆå¤šä¸ªæ–‡ä»¶ç”¨è‹±æ–‡é€—å·åˆ†éš”ï¼‰ç”¨äºï¼š{name}', 
                help='æ”¯æŒè®¿é—® /mnt/nas-search-nlp/XXX è·¯å¾„ä¸‹çš„æ•°æ®é›†ï¼Œå¦‚: /mnt/nas-search-nlp/test/train.txtã€‚'
            )
            if dataset_path_str:
                dataset_path_list = dataset_path_str.split(',')
                for dataset_path in dataset_path_list:
                    if not dataset_path:
                        continue
                    if not os.path.exists(dataset_path):
                        st.error(f'æœªæ‰¾åˆ°æ–‡ä»¶ "{dataset_path}"ï¼Œè¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„ã€‚')
                        st.stop()
                    else:
                        lines = [json.loads(line.strip()) for line in open(dataset_path, 'r', encoding='utf8').readlines()]
                        df_dict = {
                            'text': lines[:5] + ['...']
                        }
                        df = pd.DataFrame.from_dict(df_dict)
                        st.dataframe(df)
                        all_samples.extend(lines)
                        files_count += 1
        st.markdown(f'æ€»æ–‡ä»¶æ•°ï¼š**:blue[{files_count}]**')
        st.markdown(f'æ€»æ ·æœ¬æ•°ï¼š**:blue[{len(all_samples)}]**')
    return all_samples


def get_doccano_samples():
    """
    è§£ædoccanoå¯¼å‡ºçš„æ•°æ®é›†ï¼Œå¹¶æ•°æ®å¢å¼ºã€‚
    """
    st.markdown('é€‰æ‹© **:green[.jsonl]** æ•°æ®é›†ï¼ˆä»doccanoå¯¼å‡ºï¼‰ç”¨äºæ­£è´Ÿä¾‹ç”Ÿæˆï¼Œæ”¯æŒæ¥æ”¶å¤šä¸ªæ–‡ä»¶ã€‚')
    st.code('âš ï¸ æ³¨æ„ï¼šè‹¥ä¼ å…¥å¤šä¸ªæ–‡ä»¶ï¼Œå¤šä¸ªæ–‡ä»¶çš„æ•°æ®å°†è¢«èåˆã€‚')
    st.markdown('* æ•°æ®é›†ç¤ºä¾‹ï¼š')
    st.code(
        """
        [
            {
                "text": "ã€Šè‚¡ä»½åˆ¶ä¼ä¸šèµ„äº§è¯„ä¼°ã€‹æ˜¯1999å¹´ä¸­å›½äººæ°‘å¤§å­¦å‡ºç‰ˆç¤¾å‡ºç‰ˆçš„å›¾ä¹¦ï¼Œä½œè€…æ˜¯åˆ˜è§’è†œçœŸèŒç—…ç‰å¹³", 
                "entities": [
                        {"id": 53636, "label": "ä¹¦ç±", "start_offset": 1, "end_offset": 10, "text": "è‚¡ä»½åˆ¶ä¼ä¸šèµ„äº§è¯„ä¼°"}, 
                        {"id": 53637, "label": "å‡ºç‰ˆç¤¾", "start_offset": 17, "end_offset": 26, "text": "ä¸­å›½äººæ°‘å¤§å­¦å‡ºç‰ˆç¤¾"}
                    ], 
                "relations": [
                        {"id": 0, "from_id": 53636, "to_id": 53637, "type": "å‡ºç‰ˆç¤¾"}
                    ]
            },
            ...
        ]
        """, language='json'
    )
    
    with st.expander('', expanded=True):
        all_samples, files_count = [], 0
        upload_file = st.checkbox(f'ä¸Šä¼ æœ¬åœ°.jsonlæ–‡ä»¶', help='ä¸Šä¼ æœ¬åœ°çš„æ•°æ®é›†æ–‡ä»¶ã€‚')
        if upload_file:
            uploaded_files = st.file_uploader("é€‰æ‹©æ–‡ä»¶", accept_multiple_files=True)
            for uploaded_file in uploaded_files:
                stringio = StringIO(uploaded_file.getvalue().decode("utf-8"))
                string_data = stringio.read()
                cur_data = string_data.split('\n')
                all_samples.extend([json.loads(line) for line in cur_data])
                files_count += 1
                df_dict = {
                    'text': cur_data[:5] + ['...']
                }
                df = pd.DataFrame.from_dict(df_dict)
                st.dataframe(df)
        else:
            dataset_path_str = st.text_input(f'ã€Œè®­ç»ƒæ•°æ®é›†ã€çš„æœåŠ¡å™¨è·¯å¾„ï¼Œjsonlæ ¼å¼ï¼ˆå¤šä¸ªæ–‡ä»¶ç”¨è‹±æ–‡é€—å·åˆ†éš”ï¼‰', 
                help='æ”¯æŒè®¿é—® /mnt/nas-search-nlp/XXX è·¯å¾„ä¸‹çš„æ•°æ®é›†ï¼Œå¦‚: /mnt/nas-search-nlp/test/doccano.jsonlã€‚'
            )
            if dataset_path_str:
                dataset_path_list = dataset_path_str.split(',')
                for dataset_path in dataset_path_list:
                    if not dataset_path:
                        continue
                    if not os.path.exists(dataset_path):
                        st.error(f'æœªæ‰¾åˆ°æ–‡ä»¶ "{dataset_path}"ï¼Œè¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„ã€‚')
                        st.stop()
                    else:
                        lines = [json.loads(line.strip()) for line in open(dataset_path, 'r', encoding='utf8').readlines()]
                        df_dict = {
                            'text': lines[:5] + ['...']
                        }
                        df = pd.DataFrame.from_dict(df_dict)
                        st.dataframe(df)
                        all_samples.extend(lines)
                        files_count += 1
        st.markdown(f'æ€»æ–‡ä»¶æ•°ï¼š**:blue[{files_count}]**')
        st.markdown(f'æ€»æ ·æœ¬æ•°ï¼š**:blue[{len(all_samples)}]**')
    return all_samples


def get_model():
    """
    åŠ è½½æ¨¡å‹/tokenizerã€‚
    """
    with st.expander('', expanded=True):
        upload_model = st.checkbox('æ˜¯å¦ä¸Šä¼ æœ¬åœ°æ¨¡å‹', help='ä¸Šä¼ æœ¬åœ°æ¨¡å‹æ–‡ä»¶ï¼Œæ¨¡å‹ç»“æ„éœ€å’Œ[è¿™é‡Œ](https://github.com/HarderThenHarder/transformers_tasks/blob/main/UIE/model.py)ä¿æŒä¸€è‡´ã€‚')
        model, tokenizer = None, None
        if upload_model:
            uploaded_model = st.file_uploader("é€‰æ‹©æ–‡ä»¶", disabled=True)
            st.warning('æš‚ä¸æ”¯æŒä¸Šä¼ æœ¬åœ°æ–‡ä»¶ã€‚')
            st.stop()
        else:
            model_path = st.text_input('ã€Œè®­ç»ƒæ¨¡å‹ã€çš„æœåŠ¡å™¨è·¯å¾„', 
                help='æ”¯æŒè®¿é—® /mnt/nas-search-nlp/XXX è·¯å¾„ä¸‹çš„æ¨¡å‹æ–‡ä»¶ï¼Œå¦‚: /mnt/nas-search-nlp/model_best/model.ptã€‚'
            )
            tokenizer_path = st.text_input('ã€Œtokenizerã€çš„æœåŠ¡å™¨è·¯å¾„', 
                help='æ”¯æŒè®¿é—® /mnt/nas-search-nlp/XXX è·¯å¾„ä¸‹çš„ç›®å½•ï¼Œå¦‚: /mnt/nas-search-nlp/model_best/'
            )

            if model_path and tokenizer_path:
                if not os.path.exists(model_path):
                    st.error(f'æœªæ‰¾åˆ°æ–‡ä»¶ "{model_path}"ï¼Œè¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„ã€‚')
                    st.stop()
                elif not os.path.exists(tokenizer_path):
                    st.error(f'æœªæ‰¾åˆ°ç›®å½• "{tokenizer_path}"ï¼Œè¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„ã€‚')
                    st.stop()
                else:
                    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
                    with st.spinner('æ¨¡å‹åŠ è½½ä¸­ï¼Œè¯·ç¨ç­‰...'):
                        if device == 'cpu':
                            model = torch.load(model_path, map_location=torch.device('cpu'))
                        else:
                            model = torch.load(model_path).to(device)
                    model.eval()
                    st.success('ğŸ‰ æ¨¡å‹åŠ è½½æˆåŠŸ~')
        return model, tokenizer


def main():
    main_external_css = """
        <style>
            h1, h2, h3, h4, h5, h6 {color: #b59a6d !important}
        </style>
    """
    st.markdown(main_external_css, unsafe_allow_html=True)
    st.markdown("[![Typing SVG](https://readme-typing-svg.demolab.com?font=Fira+Code&duration=600&pause=500&color=38F77FFF&vCenter=true&multiline=true&width=606&height=130&lines=import+torch;from+model+import+Model;Model.fit()_;Epoch+1+[=======================================];Get+Off+Work%EF%BC%81)](https://github.com/HarderThenHarder/transformers_tasks)")

    da_page, model_analysis_page = st.tabs(['UIE-æ•°æ®å¢å¼º', 'UIE-è‡ªåˆ†æè´Ÿä¾‹ç”Ÿæˆ'])
    with da_page:
        da_mode = st.radio('æ•°æ®å¢å¼ºæ¨¡å¼', ['åŸºäºè§„åˆ™ï¼ˆSwap SPOï¼‰', 'åŸºäºæ¨¡å‹ï¼ˆMask Then Fillï¼‰'])
        if da_mode == 'åŸºäºè§„åˆ™ï¼ˆSwap SPOï¼‰':
            with st.expander('Swap SPO ç­–ç•¥ä»‹ç»', expanded=False):
                st.markdown("""
                    Swap SPO æ˜¯ä¸€ç§åŸºäºè§„åˆ™çš„ç®€å•æ•°æ®å¢å¼ºç­–ç•¥ã€‚<br>
                    
                    å°†åŒä¸€æ•°æ®é›†ä¸­ ç›¸åŒPçš„å¥å­åˆ†æˆä¸€ç»„ï¼Œå¹¶éšæœºäº¤æ¢è¿™äº›å¥å­ä¸­çš„Så’ŒOã€‚<br>
                    
                    ---

                    * è¾“å…¥ï¼š<br>
                    
                    1. :blue[ã€Šå¤œæ›²ã€‹]æ˜¯:green[å‘¨æ°ä¼¦]:violet[ä½œæ›²]çš„ä¸€é¦–æ­Œã€‚<br>

                    2. :blue[ã€Šé‚£äº›ä½ å¾ˆå†’é™©çš„æ¢¦ã€‹]æ˜¯å½“ä¸‹éå¸¸ç«çƒ­çš„ä¸€é¦–æ­Œï¼Œ:violet[ä½œæ›²]ä¸º:green[æ—ä¿Šæ°]ã€‚<br>
                    
                    ---

                    * è¾“å‡ºï¼š<br>

                    :blue[ã€Šå¤œæ›²ã€‹]æ˜¯å½“ä¸‹éå¸¸ç«çƒ­çš„ä¸€é¦–æ­Œï¼Œ:violet[ä½œæ›²]ä¸º:green[å‘¨æ°ä¼¦]ã€‚
                """, unsafe_allow_html=True)
            all_samples = get_prompt_samples(name='æ­£ä¾‹å¢å¼º')
        elif da_mode == 'åŸºäºæ¨¡å‹ï¼ˆMask Then Fillï¼‰':
            with st.expander('Mask Then Fill ç­–ç•¥ä»‹ç»', expanded=False):
                st.markdown("""
                    [Mask Then Fill](https://arxiv.org/pdf/2301.02427.pdf) æ˜¯ä¸€ç§åŸºäºç”Ÿæˆæ¨¡å‹çš„ä¿¡æ¯æŠ½å–æ•°æ®å¢å¼ºç­–ç•¥ã€‚
                    
                    å¯¹äºä¸€æ®µæ–‡æœ¬ï¼Œæˆ‘ä»¬å…¶åˆ†ä¸ºã€Œå…³é”®ä¿¡æ¯æ®µã€å’Œã€Œéå…³é”®ä¿¡æ¯æ®µã€ï¼ŒåŒ…å«å…³é”®è¯ç‰‡æ®µç§°ä¸ºã€Œå…³é”®ä¿¡æ¯æ®µã€ã€‚

                    > `Note:` ä¾‹å­ä¸­å¸¦é¢œè‰²çš„ä»£è¡¨å…³é”®ä¿¡æ¯æ®µï¼Œå…¶ä½™çš„ä¸ºéå…³é”®ä¿¡æ¯æ®µã€‚
                    
                    [e.g.] :blue[å¤§å¹´ä¸‰å]æˆ‘ä»:violet[åŒ—äº¬]çš„å¤§å…´æœºåœº:orange[é£å›]äº†:green[æˆéƒ½]ã€‚

                    ---

                    1. æˆ‘ä»¬éšæœº [MASK] ä½ä¸€éƒ¨åˆ†ã€Œéå…³é”®ç‰‡æ®µã€ï¼Œä½¿å…¶å˜ä¸ºï¼š

                    [e.g.] :blue[å¤§å¹´ä¸‰å]æˆ‘ä»:violet[åŒ—äº¬]**[MASK]**:orange[é£å›]äº†:green[æˆéƒ½]ã€‚

                    2. éšåå–‚ç»™ filling æ¨¡å‹ï¼ˆT5-Fine Tunedï¼‰è¿˜åŸå¥å­ï¼Œå¾—åˆ°æ–°ç”Ÿæˆçš„å¥å­ï¼š

                    [e.g.] :blue[å¤§å¹´ä¸‰å]æˆ‘ä»:violet[åŒ—äº¬]é¦–éƒ½æœºåœºä½œä¸ºèµ·ç‚¹ï¼Œ:orange[é£å›]äº†:green[æˆéƒ½]ã€‚

                """, unsafe_allow_html=True)
            all_samples = get_doccano_samples()
            aug_num = st.number_input('æ•°æ®å¢å¼ºå€æ•°', min_value=1, max_value=4, value=1)
        finish_button = st.button('å¼€å§‹ç”Ÿæˆæ­£ä¾‹')
        if finish_button:
            start = time.time()
            with st.spinner('æ­£ä¾‹ç”Ÿæˆä¸­ï¼Œè¯·ç¨ç­‰...'):
                if da_mode == 'åŸºäºè§„åˆ™ï¼ˆSwap SPOï¼‰':
                    positive_samples, predicates_sentence_dict = Augmenter.auto_add_uie_relation_positive_samples(
                        all_samples
                    )
                    st.session_state['positive_samples'] = positive_samples
                    st.session_state['predicates_sentence_dict'] = predicates_sentence_dict
                elif da_mode == 'åŸºäºæ¨¡å‹ï¼ˆMask Then Fillï¼‰':
                    if 'filling_model' not in st.session_state:
                        st.session_state['filling_model'] = T5ForConditionalGeneration.from_pretrained(filling_model_path).to(device)
                        st.session_state['filling_tokenizer'] = AutoTokenizer.from_pretrained(filling_model_path)
                    positive_samples = Augmenter.auto_add_uie_relation_positive_samples(
                        all_samples,
                        mode='mask_then_fill',
                        filling_model=st.session_state['filling_model'],
                        filling_tokenizer=st.session_state['filling_tokenizer'],
                        batch_size=64,
                        max_seq_len=128,
                        device=device,
                        aug_num=aug_num
                    )
                    st.session_state['positive_samples'] = positive_samples
            time_used = time.time() - start
            st.balloons()
            st.markdown('è€—æ—¶ï¼š:green[{:.2f}s]'.format(time_used))
        
        if 'positive_samples' in st.session_state:
            if da_mode == 'åŸºäºè§„åˆ™ï¼ˆSwap SPOï¼‰':
                df_dict = {
                    'content': [],
                    'prompt': [],
                    'result_list': []
                }
            elif da_mode == 'åŸºäºæ¨¡å‹ï¼ˆMask Then Fillï¼‰':
                df_dict = {
                    'text': [],
                    'entities': [],
                    'relations': []
                }
            for sample in st.session_state['positive_samples']:
                sample = json.loads(sample)
                for k, v in sample.items():
                    df_dict[k].append(str(v))

            st.markdown('---')
            st.markdown(f'ç”Ÿæˆæ­£ä¾‹æ•°ï¼š:green[{len(st.session_state["positive_samples"])}]')
            
            if da_mode == 'åŸºäºè§„åˆ™ï¼ˆSwap SPOï¼‰':
                st.markdown(f'å…±åŒ…å« predicate ä¸ªæ•°ï¼š:green[{len(st.session_state["predicates_sentence_dict"])}]')
                with st.expander('åŒ Predicate æ ·æœ¬è¯¦æƒ…ï¼ˆè‹¥æ•°æ®é›†è¾ƒå¤§å»ºè®®å…ˆã€Œä¸‹è½½ç”Ÿæˆç»“æœã€å†å±•å¼€ï¼Œä»¥é˜²å¡æ­»ï¼‰', expanded=False):
                    st.write('æ•°æ®é›†ä¸‹åŒ…å«çš„æ‰€æœ‰Predicate(s)ï¼š')
                    st.code(f'{list(st.session_state["predicates_sentence_dict"].keys())}')
                    search_p = st.selectbox('æŸ¥çœ‹æ•°æ®é›†ä¸­æŒ‡å®šçš„åŒPæ ·æœ¬ï¼š', list(st.session_state["predicates_sentence_dict"].keys()))
                    st.write(st.session_state['predicates_sentence_dict'][search_p])
            
            df = pd.DataFrame.from_dict(df_dict)
            st.dataframe(df, height=generated_dataset_height)
            text_file = '\n'.join(st.session_state['positive_samples'])
            st.download_button('ä¸‹è½½æ­£ä¾‹æ•°æ®é›†', text_file)
    
    with model_analysis_page:
        all_samples = get_prompt_samples(name='è‡ªåˆ†æè´Ÿä¾‹ç”Ÿæˆ')
        model, tokenizer = get_model()
        finish_button = st.button('å¼€å§‹ç”Ÿæˆè´Ÿä¾‹')
        if finish_button:
            start = time.time()
            with st.spinner('è´Ÿä¾‹ç”Ÿæˆä¸­ï¼Œè¯·ç¨ç­‰...'):
                predicate_error_dict, summary_dict, negative_samples = Augmenter.auto_add_uie_relation_negative_samples(
                    model,
                    tokenizer,
                    all_samples,
                    inference,
                    device=device,
                    max_seq_len=max_seq_len,
                    batch_size=batch_size
                )
                st.session_state['predicate_error_dict'] = predicate_error_dict
                st.session_state['summary_dict'] = summary_dict
                st.session_state['negative_samples'] = negative_samples
            time_used = time.time() - start
            st.balloons()
            st.markdown('è€—æ—¶ï¼š:green[{:.2f}s]'.format(time_used))
        
        if 'negative_samples' in st.session_state:
            df_dict = {
                'content': [],
                'prompt': [],
                'result_list': []
            }
            for sample in st.session_state['negative_samples']:
                if len(df_dict['content']) > max_show_num:
                    break
                sample = json.loads(sample)
                for k, v in sample.items():
                    df_dict[k].append(str(v))

            st.markdown('---')
            st.markdown(f'ç”Ÿæˆè´Ÿä¾‹æ•°ï¼š:green[{len(st.session_state["negative_samples"])}]')
            with st.expander('æ˜“æ··æ·†Predicateè¯¦æƒ…', expanded=True):
                p_df_dict = {
                    'åŸå§‹P': [],
                    'æ˜“æ··æ·†P': []
                }
                for k, v in st.session_state['summary_dict'].items():
                    p_df_dict['åŸå§‹P'].append(k)
                    p_df_dict['æ˜“æ··æ·†P'].append(','.join(v))
                pdf = pd.DataFrame.from_dict(p_df_dict)
                st.table(pdf)
                search_p = st.selectbox('æŸ¥çœ‹æŒ‡å®šPçš„æ··æ·†è¯¦æƒ…', p_df_dict['åŸå§‹P'])
                if search_p:
                    st.write(st.session_state['predicate_error_dict'][search_p])
            df = pd.DataFrame.from_dict(df_dict)
            st.dataframe(df, height=generated_dataset_height)
            text_file = '\n'.join(st.session_state["negative_samples"])
            st.download_button('ä¸‹è½½è´Ÿä¾‹æ•°æ®é›†', text_file)


if __name__ == '__main__':
    main()